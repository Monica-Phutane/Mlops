{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample dataset\n",
        "data={\n",
        "  \"english\": [\n",
        "    \"hello\", \"how are you\", \"thank you\", \"good morning\", \"good night\",\n",
        "    \"what is your name\", \"I love you\", \"see you later\", \"goodbye\", \"yes\",\n",
        "    \"no\", \"please\", \"how much is this\", \"where are you from\", \"I'm fine\",\n",
        "    \"nice to meet you\", \"have a nice day\", \"sorry\", \"I don't understand\",\n",
        "    \"can you help me\", \"what time is it\", \"I need a doctor\", \"let's go\",\n",
        "    \"I'm hungry\", \"what do you do\", \"I am a student\", \"where is the bathroom\",\n",
        "    \"it is raining\", \"I like this\", \"this is my friend\", \"what is your job\",\n",
        "    \"do you speak English\", \"I speak a little\", \"what are you doing\",\n",
        "    \"I am learning Tamil\", \"I miss you\", \"congratulations\", \"happy birthday\",\n",
        "    \"welcome\", \"good luck\", \"take care\", \"see you soon\", \"what happened\",\n",
        "    \"I will call you\", \"are you okay\", \"I am tired\", \"I want to go home\",\n",
        "    \"it's time to eat\", \"let's take a picture\", \"that's interesting\", \"I like music\"\n",
        "  ],\n",
        "  \"french\": [\n",
        "    \"<sos> నమస్కారం <eos>\",\n",
        "    \"<sos> మీరు ఎలా ఉన్నారు <eos>\",\n",
        "    \"<sos> ధన్యవాదాలు <eos>\",\n",
        "    \"<sos> శుభోదయం <eos>\",\n",
        "    \"<sos> శుభ రాత్రి <eos>\",\n",
        "    \"<sos> మీ పేరు ఏమిటి <eos>\",\n",
        "    \"<sos> నేను నిన్ను ప్రేమిస్తున్నాను <eos>\",\n",
        "    \"<sos> మళ్ళీ కలుద్దాం <eos>\",\n",
        "    \"<sos> వీడ్కోలు <eos>\",\n",
        "    \"<sos> అవును <eos>\",\n",
        "    \"<sos> లేదు <eos>\",\n",
        "    \"<sos> దయచేసి <eos>\",\n",
        "    \"<sos> ఇది ఎంత <eos>\",\n",
        "    \"<sos> మీరు ఎక్కడ నుంచి వచ్చారు <eos>\",\n",
        "    \"<sos> నేను బాగున్నాను <eos>\",\n",
        "    \"<sos> మిమ్మల్ని కలవడం ఆనందంగా ఉంది <eos>\",\n",
        "    \"<sos> మీకు మంచి రోజు <eos>\",\n",
        "    \"<sos> నన్ను క్షమించండి <eos>\",\n",
        "    \"<sos> నాకు అర్థం కాలేదు <eos>\",\n",
        "    \"<sos> మీరు నాకు సహాయం చేయగలరా <eos>\",\n",
        "    \"<sos> ఇప్పుడు సమయం ఎంత <eos>\",\n",
        "    \"<sos> నాకు డాక్టర్ కావాలి <eos>\",\n",
        "    \"<sos> వెళ్దాం <eos>\",\n",
        "    \"<sos> నాకు ఆకలిగా ఉంది <eos>\",\n",
        "    \"<sos> మీరు ఏమి చేస్తారు <eos>\",\n",
        "    \"<sos> నేను విద్యార్థిని / నేను విద్యార్థిని ని <eos>\",\n",
        "    \"<sos> బాత్‌రూమ్ ఎక్కడ ఉంది <eos>\",\n",
        "    \"<sos> వర్షం పడుతోంది <eos>\",\n",
        "    \"<sos> నాకు ఇది ఇష్టం <eos>\",\n",
        "    \"<sos> ఇది నా మిత్రుడు / ఇది నా మిత్రురాలు <eos>\",\n",
        "    \"<sos> మీ ఉద్యోగం ఏమిటి <eos>\",\n",
        "    \"<sos> మీరు ఇంగ్లీష్ మాట్లాడతారా <eos>\",\n",
        "    \"<sos> నేను కొంచెం మాట్లాడతాను <eos>\",\n",
        "    \"<sos> మీరు ఏమి చేస్తున్నారు <eos>\",\n",
        "    \"<sos> నేను తమిళం నేర్చుకుంటున్నాను <eos>\",\n",
        "    \"<sos> నేను నిన్ను మిస్ అవుతున్నాను <eos>\",\n",
        "    \"<sos> అభినందనలు <eos>\",\n",
        "    \"<sos> పుట్టిన రోజు శుభాకాంక్షలు <eos>\",\n",
        "    \"<sos> స్వాగతం <eos>\",\n",
        "    \"<sos> అదృష్టం <eos>\",\n",
        "    \"<sos> జాగ్రత్తగా ఉండండి <eos>\",\n",
        "    \"<sos> త్వరలో కలుద్దాం <eos>\",\n",
        "    \"<sos> ఏమి జరిగింది <eos>\",\n",
        "    \"<sos> నేను మీరు కి ఫోన్ చేస్తాను <eos>\",\n",
        "    \"<sos> మీరు బాగున్నారా <eos>\",\n",
        "    \"<sos> నేను అలసిపోయాను <eos>\",\n",
        "    \"<sos> నేను ఇంటికి వెళ్లాలి <eos>\",\n",
        "    \"<sos> తినప్రాంత సమయంలో <eos>\",\n",
        "    \"<sos> ఫోటో తీసుకుందాం <eos>\",\n",
        "    \"<sos> ఇది ఆసక్తికరంగా ఉంది <eos>\",\n",
        "    \"<sos> నాకు సంగీతం ఇష్టం <eos>\"\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tokenization\n",
        "english_tokenizer = Tokenizer()\n",
        "french_tokenizer = Tokenizer()\n",
        "french_tokenizer = Tokenizer(filters='')\n",
        "# Fit the tokenizers\n",
        "english_tokenizer.fit_on_texts(df['english'])\n",
        "french_tokenizer.fit_on_texts(df['french'])\n",
        "\n",
        "# Convert sentences to sequences\n",
        "english_sequences = english_tokenizer.texts_to_sequences(df['english'])\n",
        "french_sequences = french_tokenizer.texts_to_sequences(df['french'])\n",
        "\n",
        "# Pad sequences\n",
        "max_english_length = max(len(seq) for seq in english_sequences)\n",
        "max_french_length = max(len(seq) for seq in french_sequences)\n",
        "\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
        "french_padded = pad_sequences(french_sequences, maxlen=max_french_length, padding='post')\n",
        "\n",
        "# Create input-output pairs\n",
        "X = english_padded\n",
        "y = french_padded"
      ],
      "metadata": {
        "id": "Cx1umADEQw1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    def call(self, encoder_output, decoder_hidden):\n",
        "        score = tf.matmul(encoder_output, tf.expand_dims(decoder_hidden, 2))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        return tf.reduce_sum(context_vector, axis=1), attention_weights\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden, encoder_output):\n",
        "        context_vector, attention_weights = BahdanauAttention()(encoder_output, hidden)\n",
        "        x = self.embedding(x)\n",
        "        # Repeat the context vector to match the sequence length of the decoder input\n",
        "        context_vector = tf.repeat(tf.expand_dims(context_vector, 1), repeats=tf.shape(x)[1], axis=1)\n",
        "        x = tf.concat([context_vector, x], axis=-1)\n",
        "        output, hidden, _ = self.lstm(x)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden, attention_weights  # Return attention weights"
      ],
      "metadata": {
        "id": "AtQzgCw1Q0cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uua2mtS7R8GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 256\n",
        "hidden_units = 512\n",
        "batch_size = 2\n",
        "epochs = 500  # Increase for better results\n",
        "\n",
        "# Create the encoder and decoder\n",
        "encoder = Encoder(vocab_size=len(english_tokenizer.word_index) + 1, embedding_dim=embedding_dim, hidden_units=hidden_units)\n",
        "decoder = Decoder(vocab_size=len(french_tokenizer.word_index) + 1, embedding_dim=embedding_dim, hidden_units=hidden_units)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(X)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoder_output, encoder_hidden, _ = encoder(tf.convert_to_tensor(X[i:i+1]))\n",
        "            decoder_input = tf.convert_to_tensor(y[i:i+1, :-1])  # Exclude last token\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            # Correctly unpack decoder output\n",
        "            output, hidden, attention_weights = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(y[i:i+1, 1:], output)  # Exclude first token\n",
        "\n",
        "        gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VfHexUN6Q-S-",
        "outputId": "f9bc637e-d738-49f4-f978-90f4515506b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 2.0433075428009033\n",
            "Epoch 2/500, Loss: 1.7671666145324707\n",
            "Epoch 3/500, Loss: 1.7036696672439575\n",
            "Epoch 4/500, Loss: 1.6044036149978638\n",
            "Epoch 5/500, Loss: 1.4526989459991455\n",
            "Epoch 6/500, Loss: 1.3049795627593994\n",
            "Epoch 7/500, Loss: 1.1797189712524414\n",
            "Epoch 8/500, Loss: 0.9679540395736694\n",
            "Epoch 9/500, Loss: 0.7759835720062256\n",
            "Epoch 10/500, Loss: 0.6793570518493652\n",
            "Epoch 11/500, Loss: 0.6792263984680176\n",
            "Epoch 12/500, Loss: 0.43619048595428467\n",
            "Epoch 13/500, Loss: 0.5519176721572876\n",
            "Epoch 14/500, Loss: 0.3565669655799866\n",
            "Epoch 15/500, Loss: 0.5429725646972656\n",
            "Epoch 16/500, Loss: 0.4703705906867981\n",
            "Epoch 17/500, Loss: 0.3806854486465454\n",
            "Epoch 18/500, Loss: 0.3954712748527527\n",
            "Epoch 19/500, Loss: 0.31432580947875977\n",
            "Epoch 20/500, Loss: 0.2839386761188507\n",
            "Epoch 21/500, Loss: 0.32896578311920166\n",
            "Epoch 22/500, Loss: 0.3127075135707855\n",
            "Epoch 23/500, Loss: 0.14635078608989716\n",
            "Epoch 24/500, Loss: 0.20300647616386414\n",
            "Epoch 25/500, Loss: 0.3642224669456482\n",
            "Epoch 26/500, Loss: 0.3616634011268616\n",
            "Epoch 27/500, Loss: 0.14453233778476715\n",
            "Epoch 28/500, Loss: 0.17574989795684814\n",
            "Epoch 29/500, Loss: 0.04376354068517685\n",
            "Epoch 30/500, Loss: 0.11381717771291733\n",
            "Epoch 31/500, Loss: 0.1781872659921646\n",
            "Epoch 32/500, Loss: 0.021410612389445305\n",
            "Epoch 33/500, Loss: 0.022423030808568\n",
            "Epoch 34/500, Loss: 0.005648995749652386\n",
            "Epoch 35/500, Loss: 0.010720944032073021\n",
            "Epoch 36/500, Loss: 0.004330005496740341\n",
            "Epoch 37/500, Loss: 0.003455164609476924\n",
            "Epoch 38/500, Loss: 0.0033460003323853016\n",
            "Epoch 39/500, Loss: 0.0033874036744236946\n",
            "Epoch 40/500, Loss: 0.0029720342718064785\n",
            "Epoch 41/500, Loss: 0.0028632867615669966\n",
            "Epoch 42/500, Loss: 0.004913330078125\n",
            "Epoch 43/500, Loss: 0.016117248684167862\n",
            "Epoch 44/500, Loss: 0.010149026289582253\n",
            "Epoch 45/500, Loss: 0.003428648691624403\n",
            "Epoch 46/500, Loss: 0.0031371144577860832\n",
            "Epoch 47/500, Loss: 0.0026441009249538183\n",
            "Epoch 48/500, Loss: 0.0022999464999884367\n",
            "Epoch 49/500, Loss: 0.002046553883701563\n",
            "Epoch 50/500, Loss: 0.001845537917688489\n",
            "Epoch 51/500, Loss: 0.0016799801960587502\n",
            "Epoch 52/500, Loss: 0.0015402939170598984\n",
            "Epoch 53/500, Loss: 0.001420529093593359\n",
            "Epoch 54/500, Loss: 0.001316441223025322\n",
            "Epoch 55/500, Loss: 0.0012249480932950974\n",
            "Epoch 56/500, Loss: 0.0011440010275691748\n",
            "Epoch 57/500, Loss: 0.0010717990808188915\n",
            "Epoch 58/500, Loss: 0.0010069243144243956\n",
            "Epoch 59/500, Loss: 0.0009483880130574107\n",
            "Epoch 60/500, Loss: 0.0008951989584602416\n",
            "Epoch 61/500, Loss: 0.000846722221467644\n",
            "Epoch 62/500, Loss: 0.0008023066911846399\n",
            "Epoch 63/500, Loss: 0.0007614940986968577\n",
            "Epoch 64/500, Loss: 0.0007237658137455583\n",
            "Epoch 65/500, Loss: 0.0006889600190334022\n",
            "Epoch 66/500, Loss: 0.0006565424846485257\n",
            "Epoch 67/500, Loss: 0.0006264550029300153\n",
            "Epoch 68/500, Loss: 0.0005983265000395477\n",
            "Epoch 69/500, Loss: 0.0005720387562178075\n",
            "Epoch 70/500, Loss: 0.0005473095225170255\n",
            "Epoch 71/500, Loss: 0.0005241694743745029\n",
            "Epoch 72/500, Loss: 0.0005023361882194877\n",
            "Epoch 73/500, Loss: 0.0004817951121367514\n",
            "Epoch 74/500, Loss: 0.00046241280506365\n",
            "Epoch 75/500, Loss: 0.00044407034874893725\n",
            "Epoch 76/500, Loss: 0.00042666387162171304\n",
            "Epoch 77/500, Loss: 0.00041023833910003304\n",
            "Epoch 78/500, Loss: 0.0003945556818507612\n",
            "Epoch 79/500, Loss: 0.0003797053941525519\n",
            "Epoch 80/500, Loss: 0.0003656132612377405\n",
            "Epoch 81/500, Loss: 0.0003521304461173713\n",
            "Epoch 82/500, Loss: 0.0003393316292203963\n",
            "Epoch 83/500, Loss: 0.0003270977467764169\n",
            "Epoch 84/500, Loss: 0.0003154437872581184\n",
            "Epoch 85/500, Loss: 0.00030428048921748996\n",
            "Epoch 86/500, Loss: 0.0002936079108621925\n",
            "Epoch 87/500, Loss: 0.00028335175011307\n",
            "Epoch 88/500, Loss: 0.0002735864254646003\n",
            "Epoch 89/500, Loss: 0.000264178030192852\n",
            "Epoch 90/500, Loss: 0.0002552308142185211\n",
            "Epoch 91/500, Loss: 0.0002465959405526519\n",
            "Epoch 92/500, Loss: 0.00023833304294385016\n",
            "Epoch 93/500, Loss: 0.00023041227541398257\n",
            "Epoch 94/500, Loss: 0.00022274443472269922\n",
            "Epoch 95/500, Loss: 0.00021540392481256276\n",
            "Epoch 96/500, Loss: 0.00020839084754697978\n",
            "Epoch 97/500, Loss: 0.0002015859936363995\n",
            "Epoch 98/500, Loss: 0.0001950788137037307\n",
            "Epoch 99/500, Loss: 0.00018876504327636212\n",
            "Epoch 100/500, Loss: 0.00018270428699906915\n",
            "Epoch 101/500, Loss: 0.00017688165826257318\n",
            "Epoch 102/500, Loss: 0.00017128231411334127\n",
            "Epoch 103/500, Loss: 0.00016583173419348896\n",
            "Epoch 104/500, Loss: 0.00016055977903306484\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-883920540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exclude first token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_UnpackGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_UnpackGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;34m\"\"\"Gradient for unpack op.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops_stack.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    737\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;31m# checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6754\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6755\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6756\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6757\u001b[0m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001b[1;32m   6758\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(english_sentence, encoder, decoder, english_tokenizer, french_tokenizer, max_french_length):\n",
        "    english_seq = english_tokenizer.texts_to_sequences([english_sentence])\n",
        "    english_padded = pad_sequences(english_seq, maxlen=max_english_length, padding='post')\n",
        "\n",
        "    encoder_output, encoder_hidden, _ = encoder(tf.convert_to_tensor(english_padded))\n",
        "\n",
        "    decoder_input = np.array([[french_tokenizer.word_index['<sos>']]])\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    french_translation = []\n",
        "    attention_matrices = []\n",
        "\n",
        "    for _ in range(max_french_length):\n",
        "        decoder_output, decoder_hidden, attention_weights = decoder(tf.convert_to_tensor(decoder_input), decoder_hidden, encoder_output)\n",
        "        predicted_token = tf.argmax(decoder_output[0, -1]).numpy()\n",
        "\n",
        "        attention_matrices.append(attention_weights.numpy())  # Store attention weights\n",
        "\n",
        "        if predicted_token == french_tokenizer.word_index['<eos>']:\n",
        "            break\n",
        "\n",
        "        french_translation.append(predicted_token)\n",
        "        decoder_input = np.array([[predicted_token]])\n",
        "\n",
        "    # Filter out the token index 0 (padding or unknown token)\n",
        "    translated_sentence = ' '.join(french_tokenizer.index_word[idx] for idx in french_translation if idx != 0)\n",
        "    return translated_sentence, attention_matrices"
      ],
      "metadata": {
        "id": "gS-nyjlCa9dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "english_sentence = \"happy birthday\"\n",
        "translated_french, attention_matrices = translate_sentence(english_sentence, encoder, decoder, english_tokenizer, french_tokenizer, max_french_length)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"T: {translated_french}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCrkqLYySN_n",
        "outputId": "947380b1-cff8-4f7d-9cdc-9a5dede7ffe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: happy birthday\n",
            "T: పుట్టిన రోజు పుట్టిన రోజు పుట్టిన రోజు పుట్టిన రోజు పుట్టిన\n"
          ]
        }
      ]
    }
  ]
}